import scrapy
from scrapy.crawler import CrawlerProcess


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'https://quotes.toscrape.com/',
    ]


    def parse(self, response):   
        for quote in response.css('div.quote'):
            yield {
                "quote": quote.css('span.text::text').get(),
                "author": quote.css('small.author::text').get(),
                "tags": quote.css('div.tags a.tag::text').getall()
            }
        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)


process = CrawlerProcess(settings={
        "FEEDS": {
            "quotes.csv": {"format": "csv"},
        },
        "LOG_LEVEL": "ERROR"
    })
process.crawl(QuotesSpider)
process.start()



import scrapy
from scrapy.crawler import CrawlerProcess
import pandas as pd


class BookSpider(scrapy.Spider):
    name = "books"
    start_urls = ['https://books.toscrape.com/']


    def parse(self, response):
        titles = response.css("article.product_pod h3 a::attr(title)").getall()
        prices = response.css("article.product_pod p.price_color::text").getall()
        
        for title, price in zip(titles, prices):
            yield {
                "title": title,
                
"price": price
            }


        next_page = response.css('li.next a::attr(href)').get()
        if next_page:
            yield response.follow(next_page, self.parse)


process = CrawlerProcess(settings={
    "FEEDS": {
        "books.csv": {"format": "csv"},
    },
    "LOG_ENABLED": False
})


process.crawl(BookSpider)
process.start()


df = pd.read_csv("books.csv")
df.head()
